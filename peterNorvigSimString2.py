# -*- coding: utf-8 -*-
"""TextCorrectionPeterNorvig

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o3HiYNVdlizjuz58qCkxwk8LY1b1hgFE

# Peter Norvig Implementation

https://norvig.com/spell-correct.html

# Text Cleaning First
"""

import pandas as pd
import re
from collections import Counter

# jason_path = '/content/drive/MyDrive/Cal Poly/4th year/Senior Project/deep-spell-checker/'
# jason_base_path = '/content/drive/MyDrive/Cal Poly/4th year/Senior Project/'
# guneet_path = '/content/drive/MyDrive/Senior Project/deep-spell-checker/'
# guneet_base_path = '/content/drive/MyDrive/Senior Project/'
# path = jason_path
# base_path = jason_base_path

df = pd.read_csv('branded_food.csv')
# print(df['ingredients'])

def invalid_char_filter(word):
  """Filter function to detect words that do not fit our list of whitelisted characters.""" 
  re1 = re.compile(r"[^A-Z^ ^%^0-9^-]")
  if re1.search(word):
    return False
  else:
    return True

def get_cleaned_ingredients_list(df):
  """Cleans up dataframe and returns the cleaned ingredients list"""
  # Drop NA values first
  df.dropna(subset=['ingredients'], inplace=True)
  # Replace some punctuation with commas
  df['ingredients'] = df.ingredients.map(lambda x: re.sub("[\[\]*:\(\).{};]", ",", x))
  # Turn ingredients list string into list split by commas
  df['ingredients_arr'] = df.ingredients.map(lambda x: [i.strip() for i in x.split(",")])

  # Add all products' ingredients list to a single list
  all_ingredients = []
  df['ingredients_arr'].map(lambda x: all_ingredients.extend(x))
  # Remove any stray empty string values
  all_ingredients_cleaned = [i for i in all_ingredients if i != ''] 
  # Make all ingredients upper case
  all_ingredients_cleaned = [x.upper() for x in all_ingredients_cleaned]

  # Filter out any ingredients that may have random ASCII characters
  all_ingredients_valid_char = list(filter(invalid_char_filter, all_ingredients_cleaned))

  # Further filtering to limit the data we will use to train our model by length of ingredient
  max_length = 50
  min_length = 3
  all_ingredients_final = list(filter(lambda x: len(x) <= max_length and len(x) >= min_length, all_ingredients_valid_char))

  return all_ingredients_final

all_ingredients_final = get_cleaned_ingredients_list(df)
# all_ingredients_final[:100]

ingredients_count = Counter(all_ingredients_final)
# ingredients_count.most_common(10)

"""# Peter Norvig Functions"""

from simstring.feature_extractor.character_ngram import CharacterNgramFeatureExtractor
from simstring.measure.cosine import CosineMeasure
from simstring.database.dict import DictDatabase
from simstring.searcher import Searcher

# Populate database with all ingredients
db = DictDatabase(CharacterNgramFeatureExtractor(2))
for ingredient in all_ingredients_final:
  db.add(ingredient)
searcher = Searcher(db, CosineMeasure())

def probability(word, N=sum(ingredients_count.values())): 
    return ingredients_count[word] / N

# NEEDS IMPROVEMENT ON ERROR MODEL
def candidates(word):
    return searcher.search(word, 0.8)

def correction(word): 
    word_possibilities = candidates(word)
    if len(word_possibilities) <= 0:
      raise Exception('Unable to perform correction')
    else:
      return max(candidates(word), key=probability)

"""Potential to improve Peter Norvig's basic implementation:
https://github.com/pirate/spellchecker

Generating more candidates for each word (and more efficiently): https://stackoverflow.com/questions/43410332/how-to-modify-peter-norvig-spell-checker-to-get-more-number-of-suggestions-per-w
"""